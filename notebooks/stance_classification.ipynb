{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual setup of required environment variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FND_ROOT=/workspace/fnd-building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define constands and import all randomness sources first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration constants.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = 0\n",
    "\n",
    "# Global original seed for randomness reproducibility.\n",
    "OG_SEED = 30082010\n",
    "\n",
    "# Prefix for storing results.\n",
    "RUN_SUFFIX = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "# Absolute path to root folder of the repository.\n",
    "FND_ROOT=%env FND_ROOT\n",
    "\n",
    "DATA_PATH_PREFIX=f\"{FND_ROOT}/datasets/datasets-fnd-stance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize randomness sources with original seed, for full reproducibility of results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(OG_SEED)\n",
    "torch.manual_seed(OG_SEED)\n",
    "random.seed(OG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All other imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import transformers\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup papermill parameters.** The cell below must be tagged with the 'parameters' tag.\n",
    "See: https://papermill.readthedocs.io/en/latest/usage-parameterize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "\n",
    "## Must be set to True when running via papermill.\n",
    "PAPERMILL = False\n",
    "\n",
    "## Seed to be used in CLM Building mode, where only 1 seed is used.\n",
    "INITIAL_SEED = 0\n",
    "\n",
    "## Name for saving model and tokenizer in huggingface hub.\n",
    "## Requires previous login using HF_TOKEN\n",
    "## Use command: huggingface-cli login --token $HF_TOKEN\n",
    "## where HF_TOKEN must be set as an environment variable with your login token.\n",
    "SAVE_CHECKPOINT_NAME=\"<user>/<model-tokenizer-name>\"\n",
    "\n",
    "## Whether to run as standalone task or as CLM building task.\n",
    "## Options: [\"standalone\", \"clm\"]\n",
    "# Standalone task:\n",
    "#      Use train and validation sets separately, for training an validation.\n",
    "#      Test with test set.\n",
    "# CLM building task:\n",
    "#      Merge train and validation sets for training, test with test set.\n",
    "RUN_MODE = \"standalone\"\n",
    "\n",
    "## Folder to store Excel result files\n",
    "XLS_RESULTS_FOLDER = \"./\"\n",
    "\n",
    "## Set to 1 to store results in S3, set to 0 otherwise.\n",
    "# Defaults to 0.\n",
    "STORE_RESULTS_S3 = 0\n",
    "\n",
    "## Parameters for saving and uploading results.\n",
    "EXPERIMENT_NAME = \"EXP-TBD\"\n",
    "RUN_SETTING = \"-1\"\n",
    "\n",
    "# Actual parameters for running the model\n",
    "\n",
    "## Starting transformers checkpoint.\n",
    "# To use private checkpoints, user must be logged in\n",
    "# using the hugging-face-cli login method.\n",
    "CHECKPOINT = \"bert-base-uncased\" \n",
    "\n",
    "## Number of labels in the dataset\n",
    "NUM_LABELS = 4\n",
    "\n",
    "## Number of seeds to explore in standalone mode.\n",
    "SEEDS_NUM = 1\n",
    "\n",
    "## Maximum length for BERT tokens\n",
    "MAX_LEN = 200\n",
    "\n",
    "## Training parameters\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-06\n",
    "WEIGHTED_LOSS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clear CUDA cache and perform garbage collection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup CUDA device if GPU is available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{CUDA_DEVICE}\" if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(CUDA_DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure ekphrasis text preprocessor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    \n",
    "    # terms that will be normalized\n",
    "    normalize=[\n",
    "        'url', \n",
    "        'email',\n",
    "        'percent',\n",
    "        'money',\n",
    "        'phone',\n",
    "        'user',\n",
    "        'time', \n",
    "        'date',\n",
    "        'number'\n",
    "    ],\n",
    "    \n",
    "    # terms that will be annotated\n",
    "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\", 'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    # corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    # unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT-specific settings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CONFIG = {\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"ignore_mismatched_sizes\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run-specific settings, taken from constants and papermill parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"OG_SEED\": OG_SEED,\n",
    "    \"SEEDS_NUM\": SEEDS_NUM,\n",
    "    \"INITIAL_SEED\": INITIAL_SEED,\n",
    "    \"CHECKPOINT\": CHECKPOINT,\n",
    "    \"NUM_LABELS\": NUM_LABELS,    \n",
    "    \"MAX_LEN\": MAX_LEN,    \n",
    "    \"TRAIN_BATCH_SIZE\": TRAIN_BATCH_SIZE,\n",
    "    \"VALID_BATCH_SIZE\": VALID_BATCH_SIZE,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "    \"WEIGHTED_LOSS\": WEIGHTED_LOSS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RUN SETTING: {RUN_SETTING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PAPERMILL:\n",
    "    print(\"Importing plain tqdm\")\n",
    "    from tqdm import tqdm    \n",
    "else:\n",
    "    print(\"Importing auto tqdm\")\n",
    "    from tqdm.auto import tqdm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom dataaset loader and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels: Comment, Deny, Query, Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = [\"support\", \"comment\", \"deny\", \"query\"]\n",
    "encoded_labels = [0, 1, 2, 3]\n",
    "label2idx = dict(zip(idx2label, encoded_labels))\n",
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_and_labels(jsonfile):\n",
    "    triplets = []\n",
    "    with open(jsonfile) as jsonf:        \n",
    "        data = json.load(jsonf)\n",
    "        for item in data[\"Examples\"]:\n",
    "            triplets.append({\n",
    "                \"id\": item[\"id\"],\n",
    "                \"tweet_id\": item[\"tweet_id\"],\n",
    "                \"raw_text\": item[\"raw_text\"],\n",
    "                \"raw_text_prev\": item[\"raw_text_prev\"],\n",
    "                \"raw_text_src\": item[\"raw_text_src\"],\n",
    "                \"spacy_processed_text\": item[\"spacy_processed_text\"],\n",
    "                \"spacy_processed_text_prev\": item[\"spacy_processed_text_prev\"],\n",
    "                \"spacy_processed_text_src\": item[\"spacy_processed_text_src\"],\n",
    "                \"stance_label\": item[\"stance_label\"],\n",
    "                \"triplet_len\": len(item[\"raw_text\"]) + len(item[\"raw_text_prev\"]) + len(item[\"raw_text_src\"])\n",
    "            })\n",
    "\n",
    "    # Sort by triplet_len ASC\n",
    "    triplets = sorted(triplets, key=lambda t: t['triplet_len'])\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, triplets, tokenizer, max_len):\n",
    "        self.triplets = triplets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        max_length = self.max_len - 3\n",
    "        make_ids = lambda x: self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(x))\n",
    "\n",
    "        current_triplet = self.triplets[index]\n",
    "\n",
    "        text_preproc = \" \".join(text_processor.pre_process_doc(current_triplet[\"raw_text\"]))\n",
    "        prev_preproc = \" \".join(text_processor.pre_process_doc(current_triplet[\"raw_text_prev\"]))\n",
    "        src_preproc = \" \".join(text_processor.pre_process_doc(current_triplet[\"raw_text_src\"]))\n",
    "        \n",
    "        text = make_ids(text_preproc)\n",
    "        prev = make_ids(prev_preproc)\n",
    "        src = make_ids(src_preproc)\n",
    "        \n",
    "        label = current_triplet[\"stance_label\"]\n",
    "\n",
    "        segment_A = src + prev\n",
    "        segment_B = text\n",
    "\n",
    "        text_ids = [tokenizer.vocab[\"[CLS]\"]] + segment_A + \\\n",
    "                   [tokenizer.vocab[\"[SEP]\"]] + segment_B + [tokenizer.vocab[\"[SEP]\"]]\n",
    "\n",
    "        # truncate if exceeds max length\n",
    "        if len(text_ids) > max_length:\n",
    "            segment_A = segment_A[0:(max_length // 2)]\n",
    "            text_ids = [tokenizer.vocab[\"[CLS]\"]] + segment_A + \\\n",
    "                       [tokenizer.vocab[\"[SEP]\"]] + segment_B + [tokenizer.vocab[\"[SEP]\"]]\n",
    "            if len(text_ids) > max_length:\n",
    "                # Truncate also segment B\n",
    "                segment_B = segment_B[0:(max_length // 2)]\n",
    "                text_ids = [tokenizer.vocab[\"[CLS]\"]] + segment_A + \\\n",
    "                           [tokenizer.vocab[\"[SEP]\"]] + segment_B + [tokenizer.vocab[\"[SEP]\"]]\n",
    "                \n",
    "        token_type_ids = [0] * (len(segment_A)+2) + [1] * (len(segment_B) + 1)\n",
    "        attention_mask = [1] * len(token_type_ids)\n",
    "        \n",
    "        padded_ids = (text_ids + ([0] * (self.max_len - len(text_ids))))\n",
    "        padded_mask = attention_mask + ([0] * (self.max_len - len(attention_mask)))\n",
    "        padded_token_type_ids = token_type_ids + ([0] * (self.max_len - len(token_type_ids)))\n",
    "\n",
    "        assert len(padded_ids) == len(padded_mask) == len(padded_token_type_ids)\n",
    "        assert len(padded_ids) <= 512\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(padded_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(padded_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(padded_token_type_ids, dtype=torch.long),\n",
    "            'stance_label': torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEXTS_FILE = f\"{DATA_PATH_PREFIX}/butfit_rumoureval2019_all/train/train.json\"\n",
    "VALIDATION_TEXTS_FILE = f\"{DATA_PATH_PREFIX}/butfit_rumoureval2019_all/dev/dev.json\"\n",
    "TEST_TEXTS_FILE = f\"{DATA_PATH_PREFIX}/butfit_rumoureval2019_all/test/test_with_labels.json\"\n",
    "TEST_TWITTER_TEXTS_FILE = f\"{DATA_PATH_PREFIX}/butfit_rumoureval2019_all/test/test_with_labels_twitter.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_triplets_per_label(triplets):\n",
    "    for label in idx2label:\n",
    "        idx = label2idx[label]\n",
    "        qty = len(list(filter(lambda x: x['stance_label'] == idx, triplets)))\n",
    "        print(f\"Label {idx} ({label}): {qty}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_train = make_text_and_labels(TRAIN_TEXTS_FILE)\n",
    "print(len(triplets_train))\n",
    "show_triplets_per_label(triplets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_val = make_text_and_labels(VALIDATION_TEXTS_FILE)\n",
    "print(len(triplets_val))\n",
    "show_triplets_per_label(triplets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_test = make_text_and_labels(TEST_TEXTS_FILE)\n",
    "print(len(triplets_test))\n",
    "show_triplets_per_label(triplets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_test_twitter = make_text_and_labels(TEST_TWITTER_TEXTS_FILE)\n",
    "print(len(triplets_test_twitter))\n",
    "show_triplets_per_label(triplets_test_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLM Building Mode Only: Merge train and validation data into train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODE == \"clm\":\n",
    "    triplets_train = triplets_train + triplets_val\n",
    "    print(len(triplets_train))\n",
    "    show_triplets_per_label(triplets_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': settings[\"TRAIN_BATCH_SIZE\"],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "validate_params = {\n",
    "    'batch_size': settings[\"TRAIN_BATCH_SIZE\"],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "test_params = {\n",
    "    'batch_size': settings[\"VALID_BATCH_SIZE\"],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training, test, and validation procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weight vector taken from BUT-FIT implementations.\n",
    "# weights = [3.8043243885040283, 1.0, 9.309523582458496, 8.90886116027832]\n",
    "# These weights are normalized so loss on class 1 have a value of 1.\n",
    "LOSS_WEIGHTS = np.array(class_weight.compute_class_weight(\n",
    "    \"balanced\",\n",
    "    classes=[0, 1, 2, 3],\n",
    "    y=list(map(lambda t: t[\"stance_label\"], triplets_train))\n",
    "))\n",
    "print(\"Got loss weights: \", LOSS_WEIGHTS)\n",
    "\n",
    "# Normalize weights wrt minimum loss value\n",
    "LOSS_WEIGHTS = LOSS_WEIGHTS / np.min(LOSS_WEIGHTS)\n",
    "print(\"Got normalized loss weights: \", LOSS_WEIGHTS)\n",
    "\n",
    "loss_fn = None\n",
    "\n",
    "# Using this requires extra work because weights are only at mini-batch level\n",
    "# https://stackoverflow.com/questions/67639540/pytorch-cross-entropy-loss-weights-not-working/67639895#67639895\n",
    "if settings[\"WEIGHTED_LOSS\"]:\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(LOSS_WEIGHTS).to(device))\n",
    "else:\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "assert loss_fn is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, training_loader, optimizer = None, scheduler = None):\n",
    "    \n",
    "    model.train()    \n",
    "    loss_acum = 0\n",
    "    N = 0\n",
    "    \n",
    "    for iters , data in tqdm(enumerate(training_loader, 0), total = len(training_loader)):\n",
    "\n",
    "        # Reset optimizer gradients\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Get model input from custom dataset        \n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)        \n",
    "        targets = data['stance_label'].to(device, dtype = torch.long)\n",
    "\n",
    "        # Make forward and backward passes in the model\n",
    "        outputs = model(ids, attention_mask=mask, token_type_ids=token_type_ids)        \n",
    "        logits = outputs[\"logits\"]\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute accumulated loss for reporting progress\n",
    "        loss_acum += loss.item()\n",
    "        if not settings[\"WEIGHTED_LOSS\"]:\n",
    "            N = N + 1\n",
    "        else:\n",
    "            N_update = sum([loss_fn.weight[k].item() for k in data['stance_label']])\n",
    "            N = N + N_update\n",
    "\n",
    "        # Update optimizer and scheduler, if any\n",
    "        if optimizer:\n",
    "            optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()        \n",
    "\n",
    "    return loss_acum / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testing_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    loss_acum=0\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    N = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for iters, data in tqdm(enumerate(testing_loader, 0),total = len(testing_loader)):\n",
    "            # Get model input from custom dataset\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['stance_label'].to(device, dtype = torch.long)\n",
    "\n",
    "            # Make forward pass for prediction\n",
    "            outputs = model(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "            logits = outputs[\"logits\"]\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            # Compute accumulated loss for reporting progress\n",
    "            loss_acum += loss.item()\n",
    "            if not settings[\"WEIGHTED_LOSS\"]:\n",
    "                N = N + 1\n",
    "            else:\n",
    "                N_update = sum([loss_fn.weight[k].item() for k in data['stance_label']])\n",
    "                N = N + N_update\n",
    "\n",
    "            # Compute expected outputs vs model outputs for reporting progress\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.softmax(logits, dim=1).cpu().detach().numpy())\n",
    "    \n",
    "    return loss_acum / N, np.array(fin_outputs), np.array(fin_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate `settings[\"SEEDS_NUM\"]` seeds to explore model performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.random.randint(0, 42069, size=settings[\"SEEDS_NUM\"])\n",
    "seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If running in CLM building mode, set initial seed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODE == \"clm\":\n",
    "    seeds = [settings[\"INITIAL_SEED\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataframe to tabulate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(settings.keys())\n",
    "column_names = column_names + [\"seed\"]\n",
    "column_names = column_names + [\"epoch\"]\n",
    "column_names = column_names + [\"train_loss\"]\n",
    "column_names = column_names + [\"val_loss\", \"val_accuracy\", \"val_f1_score_micro\", \"val_f1_score_macro\"]\n",
    "column_names = column_names + [\"test_loss\", \"test_accuracy\", \"test_f1_score_micro\", \"test_f1_score_macro\"]\n",
    "column_names = column_names + [\"test_twitter_loss\", \"test_twitter_accuracy\", \"test_twitter_f1_score_micro\", \"test_twitter_f1_score_macro\"]\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results = pd.DataFrame(columns=column_names)\n",
    "run_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best model across seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "best_valid_accuracy = 0\n",
    "best_valid_f1_macro = 0\n",
    "best_val_loss_epoch = None\n",
    "\n",
    "_checkpoint = settings[\"CHECKPOINT\"]\n",
    "_num_labels = settings[\"NUM_LABELS\"]\n",
    "_max_length = settings[\"MAX_LEN\"]\n",
    "_learning_rate = settings[\"LEARNING_RATE\"]\n",
    "_epochs = settings[\"EPOCHS\"]\n",
    "\n",
    "for seed in seeds:\n",
    "\n",
    "    # ###########################################\n",
    "    # ! Setup randomness to use current seed\n",
    "    print(f\"Seed: {seed}\")\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    # ###########################################\n",
    "\n",
    "    print(\"Train dataset statistics\")\n",
    "    print(\"Total Tweets: \", len(triplets_train))\n",
    "    print(\"Label 0 (support): \", len(list(filter(lambda x: x['stance_label'] == 0, triplets_train))))\n",
    "    print(\"Label 1 (comment): \", len(list(filter(lambda x: x['stance_label'] == 1, triplets_train))))\n",
    "    print(\"Label 2 (deny): \", len(list(filter(lambda x: x['stance_label'] == 2, triplets_train))))\n",
    "    print(\"Label 3 (query): \", len(list(filter(lambda x: x['stance_label'] == 3, triplets_train))))\n",
    "\n",
    "    print(f\"Tokenizer: {_checkpoint}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(_checkpoint)\n",
    "    \n",
    "    training_set = CustomDataset(triplets_train, tokenizer, _max_length)    \n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    \n",
    "    validation_set = CustomDataset(triplets_val, tokenizer, _max_length)\n",
    "    validation_loader = DataLoader(validation_set, **validate_params)\n",
    "\n",
    "    test_set = CustomDataset(triplets_test, tokenizer, _max_length)\n",
    "    test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "    test_twitter_set = CustomDataset(triplets_test_twitter, tokenizer, _max_length)\n",
    "    test_twitter_loader = DataLoader(test_twitter_set, **test_params)\n",
    "    \n",
    "    # Load pretrained model\n",
    "    print(f\"Model from: {_checkpoint} with {_num_labels} labels\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        _checkpoint,\n",
    "        num_labels=_num_labels, **BERT_CONFIG\n",
    "    )\n",
    "\n",
    "    print(\"Model config:\")\n",
    "    print(model.config)\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params =  model.parameters(),\n",
    "        lr=_learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    scheduler = None    \n",
    "    \n",
    "    for epoch in range(_epochs):\n",
    "        print(f\"===== EPOCH {epoch} / SEED {seed}\")\n",
    "\n",
    "        # ######################################################################################\n",
    "        # Train\n",
    "        train_loss = train(epoch, model, training_loader, optimizer, scheduler)\n",
    "\n",
    "        # ######################################################################################\n",
    "        # Validation: only in standalone task mode.\n",
    "        if RUN_MODE == \"standalone\":        \n",
    "            val_loss, val_outputs, val_targets = validation(model, validation_loader)\n",
    "            \n",
    "            val_outputs_bin = np.argmax(val_outputs, axis=1)\n",
    "            val_accuracy = metrics.accuracy_score(val_targets, val_outputs_bin)\n",
    "            val_f1_score_micro = metrics.f1_score(val_targets, val_outputs_bin, average='micro')\n",
    "            val_f1_score_macro = metrics.f1_score(val_targets, val_outputs_bin, average='macro')\n",
    "    \n",
    "            print(\"Validation Outputs: \")\n",
    "            print(val_outputs)\n",
    "            \n",
    "            print(\"Validation Outputs bin: \")\n",
    "            print(val_outputs_bin)\n",
    "            \n",
    "            if best_valid_loss > val_loss:\n",
    "                best_valid_loss = val_loss\n",
    "                best_val_loss_epoch = epoch\n",
    "                print(f\"Best val loss: {best_valid_loss} at epoch {epoch}\")\n",
    "    \n",
    "            print(f\"Validatiton Accuracy Score = {val_accuracy}\")\n",
    "            print(f\"Validation F1 Score (Micro) = {val_f1_score_micro}\")\n",
    "            print(f\"Validation F1 Score (Macro) = {val_f1_score_macro}\")\n",
    "            print(f'Train loss: {train_loss}\\t Validation loss:{val_loss}')\n",
    "        else:\n",
    "            val_loss = -1\n",
    "            val_accuracy = -1\n",
    "            val_f1_score_micro = -1\n",
    "            val_f1_score_macro = -1            \n",
    "\n",
    "        # ######################################################################################\n",
    "        # Test with full test set\n",
    "        test_loss, test_outputs, test_targets = validation(model, test_loader)\n",
    "        \n",
    "        test_outputs_bin = np.argmax(test_outputs, axis=1)\n",
    "        test_accuracy = metrics.accuracy_score(test_targets, test_outputs_bin)\n",
    "        test_f1_score_micro = metrics.f1_score(test_targets, test_outputs_bin, average='micro')\n",
    "        test_f1_score_macro = metrics.f1_score(test_targets, test_outputs_bin, average='macro')\n",
    "\n",
    "        print(f\"Test Accuracy Score = {test_accuracy}\")\n",
    "        print(f\"Test F1 Score (Micro) = {test_f1_score_micro}\")\n",
    "        print(f\"Test F1 Score (Macro) = {test_f1_score_macro}\")\n",
    "        print(f\"Test loss: {test_loss}\")\n",
    "\n",
    "        # ######################################################################################\n",
    "        # Test with Twitter test set\n",
    "        test_twitter_loss, test_twitter_outputs, test_twitter_targets = validation(model, test_twitter_loader)\n",
    "        \n",
    "        test_twitter_outputs_bin = np.argmax(test_twitter_outputs, axis=1)\n",
    "        test_twitter_accuracy = metrics.accuracy_score(test_twitter_targets, test_twitter_outputs_bin)\n",
    "        test_twitter_f1_score_micro = metrics.f1_score(test_twitter_targets, test_twitter_outputs_bin, average='micro')\n",
    "        test_twitter_f1_score_macro = metrics.f1_score(test_twitter_targets, test_twitter_outputs_bin, average='macro')\n",
    "\n",
    "        print(f\"Test Twitter Accuracy Score = {test_twitter_accuracy}\")\n",
    "        print(f\"Test Twitter F1 Score (Micro) = {test_twitter_f1_score_micro}\")\n",
    "        print(f\"Test Twitter F1 Score (Macro) = {test_twitter_f1_score_macro}\")\n",
    "        print(f\"Test Twitter loss: {test_twitter_loss}\")\n",
    "\n",
    "        results_row = {\n",
    "            **settings,\n",
    "            'seed': seed,\n",
    "            'epoch': epoch,\n",
    "            'run_mode': RUN_MODE,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_f1_score_micro': val_f1_score_micro,\n",
    "            'val_f1_score_macro': val_f1_score_macro,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_f1_score_micro': test_f1_score_micro,\n",
    "            'test_f1_score_macro': test_f1_score_macro,\n",
    "            'test_twitter_loss': test_twitter_loss,\n",
    "            'test_twitter_accuracy': test_twitter_accuracy,\n",
    "            'test_twitter_f1_score_micro': test_twitter_f1_score_micro,\n",
    "            'test_twitter_f1_score_macro': test_twitter_f1_score_macro,\n",
    "        }\n",
    "\n",
    "        run_results = pd.concat([run_results, pd.DataFrame([results_row])], ignore_index=True)\n",
    "\n",
    "    # Save model to huggingface model hub. Only in CLM Building mode.\n",
    "    if RUN_MODE == \"clm\":\n",
    "        model.push_to_hub(SAVE_CHECKPOINT_NAME, private=True)\n",
    "        tokenizer.push_to_hub(SAVE_CHECKPOINT_NAME, private=True)\n",
    "            \n",
    "    del model    \n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PAPERMILL:\n",
    "    # Store results file in Excel\n",
    "    EXCEL_OUTPUT = f\"{EXPERIMENT_NAME}_SETTING_{RUN_SETTING:02d}_RUN_{RUN_SUFFIX}.xlsx\"\n",
    "    run_results.to_excel(f\"{XLS_RESULTS_FOLDER}/{EXCEL_OUTPUT}\")\n",
    "\n",
    "    if STORE_RESULTS_S3 == 1:\n",
    "        # Upload results to S3. Depends on environment variables.\n",
    "        # We use $$ to force environment variable and to be able to combine it with local variable {EXCEL_OUTPUT}\n",
    "        !aws s3 cp {XLS_RESULTS_FOLDER}/{EXCEL_OUTPUT} s3://$$S3_BUCKET/ --endpoint-url=$$S3_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"END\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
