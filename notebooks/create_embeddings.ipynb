{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manually set environment variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FND_ROOT=/workspace/fnd-building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set randomness sources first with original seed, for full reproducibility of results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_SEED = 30082010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(OG_SEED)\n",
    "torch.manual_seed(OG_SEED)\n",
    "random.seed(OG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from TweetUtils import TweetUtils\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration constants.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = 0\n",
    "FND_ROOT=%env FND_ROOT\n",
    "TWITTER16_ROOT=f'{FND_ROOT}/datasets/datasets-fnd-tweet16'\n",
    "TWITTER16_LABEL_RN_ROOT=f'{FND_ROOT}/notebooks'\n",
    "TAG = 'RN'\n",
    "LABEL_FILE = 'labelRN.txt'\n",
    "SEQ_PADDING = True\n",
    "\n",
    "EMBEDDINGS_ROOT = f\"{FND_ROOT}/experiments/embeddings\"\n",
    "\n",
    "RUN_SUFFIX = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clear cuda cache and perform garbage collection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup CUDA device if GPU is available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{CUDA_DEVICE}\" if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(CUDA_DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure ekphrasis text preprocessor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    \n",
    "    # terms that will be normalized\n",
    "    normalize=[\n",
    "        'url',\n",
    "        'email',\n",
    "        'percent',\n",
    "        'money',\n",
    "        'phone',\n",
    "        'user', \n",
    "        'time',\n",
    "        'date',\n",
    "        'number'\n",
    "    ],\n",
    "    \n",
    "    # terms that will be annotated\n",
    "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\", 'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    # corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    # unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load auxiliary class to traverse the Twitter16 dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetUtils  = TweetUtils(TWITTER16_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup papermill parameters.** The cell below must be tagged with the 'parameters' tag. See: https://papermill.readthedocs.io/en/latest/usage-parameterize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "\n",
    "## Must be set to True when runnin via papermill.\n",
    "PAPERMILL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"CHECKPOINT\": \"bert-base-uncased\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PAPERMILL:\n",
    "    print(\"Importing plain tqdm\")\n",
    "    from tqdm import tqdm    \n",
    "else:\n",
    "    print(\"Importing auto tqdm\")\n",
    "    from tqdm.auto import tqdm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_posts,\n",
    "    labeled_posts,\n",
    "    number_of_tweets,\n",
    "    tree_max_num_seq\n",
    ") = tweetUtils.loadAllPosts(LABEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels: rumor, non-rumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rumor', 'non-rumor']\n",
    "encoded_labels = [0, 1]\n",
    "idx2label = categories\n",
    "label2idx = dict(zip(idx2label, encoded_labels))\n",
    "num_categories = len(categories)\n",
    "\n",
    "print(f\"Number of categories: {num_categories}\")\n",
    "print(f\"Labels to indices: {label2idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples_per_label():\n",
    "    label_list = [v[0] for k,v in labeled_posts.items()]\n",
    "    for label in idx2label:\n",
    "        idx = label2idx[label]\n",
    "        qty = len(list(filter(lambda l: l == label, label_list)))\n",
    "        print(f\"Label {idx} ({label}): {qty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_examples_per_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDER = None\n",
    "EMBEDDER_STRATEGY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_XY_with_BERT(\n",
    "    _all_posts,\n",
    "    _emb_size,\n",
    "    _number_of_tweets,\n",
    "    _labeled_posts,\n",
    "    _tree_max_num_seq,\n",
    "    _categories,\n",
    "    _bert_tokenizer,\n",
    "    _bert_model,\n",
    "    _sentence_model\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate X, Y matrices with embeddings ready to be applied on a neural netowrk.\n",
    "    In addition, it returns the list of words in post that are not found\n",
    "    in the given vocabulary.\n",
    "\n",
    "    It relies on global variables EMBEDDER and EMBEDDER_STRATEGY:\n",
    "    \n",
    "    - if EMBEDDER is 'RAW_BERT', then the _bert_tokenizer and _bert_model parameters are used\n",
    "    to generate the embeddings.\n",
    "\n",
    "    - if EMBEDDER is 'SENTENCE_BERT': the _sentence_model parameter is used to generate\n",
    "    the embeddings.    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X, Y, words_not_in_model)\n",
    "    \"\"\"\n",
    " \n",
    "\n",
    "    def to_category_vector(_category, _categories):\n",
    "        vector = np.zeros(len(_categories)).astype(np.float32)\n",
    "        for i in range(len(_categories)):\n",
    "            if _categories[i] == _category:\n",
    "                vector[i] = 1.0\n",
    "                break\n",
    "        return vector\n",
    "    \n",
    "    def padOrTruncate(empty_tensor, max_num, orig_tensor):        \n",
    "        if not SEQ_PADDING:\n",
    "            return orig_tensor\n",
    "\n",
    "        len_orig_tensor = orig_tensor.size(0)\n",
    "\n",
    "        if len_orig_tensor > max_num:\n",
    "            # Truncate\n",
    "            result = orig_tensor[:max_num]\n",
    "        elif len_orig_tensor <= max_num:\n",
    "            # Pad\n",
    "            repeats = empty_tensor.repeat(max_num - len(orig_tensor), 1)\n",
    "            result = torch.cat((orig_tensor, repeats))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generateBERTEmbedding(docTexts):\n",
    "        global EMBEDDER\n",
    "        global EMBEDDER_STRATEGY\n",
    "        result = []\n",
    "        \n",
    "        for t in docTexts:\n",
    "            preprocessed_text = \" \".join(text_processor.pre_process_doc(t))          \n",
    "\n",
    "            # Default strategy: RAW-BERT\n",
    "            if EMBEDDER is None:\n",
    "                EMBEDDER = \"RAW_BERT\"\n",
    "\n",
    "            assert EMBEDDER in [\"RAW_BERT\", \"SENTENCE_BERT\"]\n",
    "\n",
    "            if EMBEDDER == \"RAW_BERT\":\n",
    "                tokenized = _bert_tokenizer.encode_plus(\n",
    "                    preprocessed_text,\n",
    "                    padding=False,\n",
    "                    truncation=True,\n",
    "                )\n",
    "\n",
    "                ids = torch.LongTensor(tokenized[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "                mask = torch.LongTensor(tokenized[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "                type_ids = (\n",
    "                    torch.LongTensor(tokenized[\"token_type_ids\"]).unsqueeze(0).to(device)\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    out = _bert_model(\n",
    "                        input_ids=ids,\n",
    "                        attention_mask=mask,\n",
    "                        token_type_ids=type_ids\n",
    "                    )\n",
    "                hidden_states = out[\"hidden_states\"]\n",
    "\n",
    "                # Default strategy for RAW-BERT embedder: embedding\n",
    "                if EMBEDDER_STRATEGY is None:\n",
    "                    EMBEDDER_STRATEGY = \"embedding\"\n",
    "\n",
    "                assert EMBEDDER_STRATEGY in [\n",
    "                    \"embedding\",\n",
    "                    \"pooler\", \n",
    "                    \"second_to_last\", \n",
    "                    \"sum_four_last\"\n",
    "                ]\n",
    "\n",
    "                if EMBEDDER_STRATEGY == \"embedding\":    \n",
    "                    ## Use embedding output layer\n",
    "                    embedding = hidden_states[0].cpu().detach()\n",
    "                    mean_embedding = torch.mean(embedding, dim=1)\n",
    "                    result.append(mean_embedding)\n",
    "                elif EMBEDDER_STRATEGY == \"pooler\":    \n",
    "                    ## Use pooler output\n",
    "                    embedding = out[\"pooler_output\"]\n",
    "                    result.append(embedding)\n",
    "                elif EMBEDDER_STRATEGY == \"second_to_last\":    \n",
    "                    ## Second-to-last hidden layer\n",
    "                    embedding = hidden_states[2].cpu().detach()\n",
    "                    mean_embedding = torch.mean(embedding, dim=1)\n",
    "                    result.append(mean_embedding)\n",
    "                elif EMBEDDER_STRATEGY == \"sum_four_last\":    \n",
    "                    ## Sum last four hidden\n",
    "                    last_four_layers = [torch.mean(hidden_states[i], dim=1) for i in (-1, -2, -3, -4)]\n",
    "                    tensor_last_four_layers = torch.stack(last_four_layers).squeeze(1)                    \n",
    "                    sum_hidden_states = torch.sum(tensor_last_four_layers, dim=0).cpu().detach()\n",
    "                    result.append(sum_hidden_states)\n",
    "\n",
    "            elif EMBEDDER == \"SENTENCE_BERT\":\n",
    "                # Default strategy for SENTENCE-BERT embedder: default\n",
    "                if EMBEDDER_STRATEGY is None:\n",
    "                    EMBEDDER_STRATEGY = \"default\"\n",
    "                \n",
    "                assert EMBEDDER_STRATEGY in [\"default\"]\n",
    "                embedding = torch.Tensor(sentence_model.encode(preprocessed_text)).to(device)\n",
    "                result.append(embedding)                \n",
    "\n",
    "        result = torch.stack(result).squeeze(1).cpu().detach()\n",
    "        return result\n",
    "\n",
    "    #def computeTreeAWE(tree, _model, _model_vocab, _emb_size):\n",
    "    #    \"\"\"Linearize a propagation tree into an embedding.\"\"\"        \n",
    "    #    return list(\n",
    "    #        map(\n",
    "    #            lambda t: [\n",
    "    #                t[0],\n",
    "    #                computeDocumentAWE(\n",
    "    #                    _all_posts[t[1]][\"text\"], _model, _model_vocab, _emb_size\n",
    "    #                ),\n",
    "    #                t[2],\n",
    "    #            ],\n",
    "    #            tree,\n",
    "    #        )\n",
    "    #    )\n",
    "\n",
    "    empty_tensor = torch.zeros([1, _emb_size])\n",
    "    _num_categories = len(_categories)\n",
    "\n",
    "    ## Compute embeddings for each propagation tree.\n",
    "    labeled_posts_awe = {\n",
    "        k: (\n",
    "            v[0],\n",
    "            generateBERTEmbedding(list(map(lambda x: _all_posts[x[1]][\"text\"], v[1]))),\n",
    "        )\n",
    "        for k, v in _labeled_posts.items()\n",
    "    }\n",
    "\n",
    "    ## pad or truncate each embedding.\n",
    "    padded_labeled_posts_awe = {\n",
    "        k: (v[0], padOrTruncate(empty_tensor, _tree_max_num_seq, v[1]))\n",
    "        for k, v in labeled_posts_awe.items()\n",
    "    }\n",
    "\n",
    "    # Generate X and Y data for feeding neural models.\n",
    "    # Both matrices are first initializad with zeros, giving them the proper shape.\n",
    "    X = np.zeros(shape=(_number_of_tweets, _tree_max_num_seq, _emb_size)).astype(\n",
    "        np.float32\n",
    "    )\n",
    "    Y = np.zeros(shape=(_number_of_tweets, _num_categories)).astype(np.float32)\n",
    "\n",
    "    # Assign data to X, up to _tree_max_num_seq elementos.\n",
    "    for idx, (tweet_id, tweet_data) in enumerate(\n",
    "        list(padded_labeled_posts_awe.items())\n",
    "    ):\n",
    "        for jdx, tweet_d in enumerate(tweet_data[1]):\n",
    "            if jdx == _tree_max_num_seq:\n",
    "                break\n",
    "            else:\n",
    "                X[idx, jdx, :] = tweet_d\n",
    "\n",
    "    # Assign data to Y.\n",
    "    for idx, (tweet_id, tweet_data) in enumerate(\n",
    "        list(padded_labeled_posts_awe.items())\n",
    "    ):\n",
    "        Y[idx, :] = to_category_vector(tweet_data[0], _categories)\n",
    "\n",
    "    print(\"X.shape: \", np.shape(X))\n",
    "    print(\"Y.shape: \", np.shape(Y))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [\n",
    "    (\"bert-base-multilingual-uncased\", \"BASE0\"),\n",
    "    (\"eprovidel/CLM01_v2_BotEN\", \"CLM01\"),\n",
    "    (\"eprovidel/CLM02_v2_BotES\", \"CLM02\"),\n",
    "    (\"eprovidel/CLM03_v2_StanceEN\", \"CLM03\"),\n",
    "    (\"eprovidel/CLM04_v2_BotEN_BotES\", \"CLM04\"),\n",
    "    (\"eprovidel/CLM05_v2_BotES_BotEN\", \"CLM05\"),\n",
    "    (\"eprovidel/CLM06_v2_BotEN_StanceEN\", \"CLM06\"),\n",
    "    (\"eprovidel/CLM07_v2_BotES_StanceEN\", \"CLM07\"),\n",
    "    (\"eprovidel/CLM08_v2_StanceEN_BotEN\", \"CLM08\"),\n",
    "    (\"eprovidel/CLM09_v2_StanceEN_BotES\", \"CLM09\"),\n",
    "    (\"eprovidel/CLM10_v2_BotEN_BotES_StanceEN\", \"CLM10\"),\n",
    "    (\"eprovidel/CLM11_v2_BotES_BotEN_StanceEN\", \"CLM11\"),\n",
    "    (\"eprovidel/CLM12_v2_BotEN_StanceEN_BotES\", \"CLM12\"),\n",
    "    (\"eprovidel/CLM13_v2_BotES_StanceEN_BotEN\", \"CLM13\"),\n",
    "    (\"eprovidel/CLM14_v2_StanceEN_BotEN_BotES\", \"CLM14\"),\n",
    "    (\"eprovidel/CLM15_v2_StanceEN_BotES_BotEN\", \"CLM15\"),\n",
    "]\n",
    "\n",
    "embedders_strategy = [\n",
    "    (\"RAW_BERT\", \"embedding\"),\n",
    "    (\"RAW_BERT\", \"pooler\"),\n",
    "    (\"RAW_BERT\", \"second_to_last\"),\n",
    "    (\"RAW_BERT\", \"sum_four_last\"),\n",
    "    (\"SENTENCE_BERT\", \"default\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _embedder, _strategy in embedders_strategy:\n",
    "    EMBEDDER = _embedder\n",
    "    EMBEDDER_STRATEGY = _strategy\n",
    "    print(\">\" * 80)\n",
    "    print(f\"Embedder: {_embedder} with strategy: {_strategy}\")\n",
    "    \n",
    "    for _checkpoint, _shortcheckpoint in checkpoints:        \n",
    "        SAVE_SUFFIX = f\"{_shortcheckpoint}_{TAG}_{EMBEDDER}_{EMBEDDER_STRATEGY}\"\n",
    "        print(\"#\" * 60)\n",
    "        print(f\"Creating embedding: {SAVE_SUFFIX}\")\n",
    "\n",
    "        emb_size = None\n",
    "\n",
    "        sentence_model = None\n",
    "        if EMBEDDER == 'SENTENCE_BERT':\n",
    "            word_embedding_model = models.Transformer(_checkpoint, max_seq_length=512)            \n",
    "            emb_size = word_embedding_model._modules['auto_model'].config.hidden_size            \n",
    "            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "            sentence_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "        bert_model = None\n",
    "        bert_tokenizer = None        \n",
    "        if EMBEDDER == 'RAW_BERT':    \n",
    "            bert_tokenizer = AutoTokenizer.from_pretrained(_checkpoint)\n",
    "            bert_model = AutoModel.from_pretrained(_checkpoint, output_hidden_states=True)\n",
    "            bert_model.to(device);\n",
    "            emb_size = bert_model.config.hidden_size\n",
    "        \n",
    "        print(\"Recomputing Xy\")\n",
    "        X, y = generate_XY_with_BERT(\n",
    "            all_posts,\n",
    "            emb_size,\n",
    "            number_of_tweets,\n",
    "            labeled_posts,\n",
    "            tree_max_num_seq,\n",
    "            categories,\n",
    "            bert_tokenizer,\n",
    "            bert_model,\n",
    "            sentence_model,\n",
    "        )\n",
    "        np.save(f\"{EMBEDDINGS_ROOT}/X_{SAVE_SUFFIX}.npy\", X, allow_pickle=False)\n",
    "        np.save(f\"{EMBEDDINGS_ROOT}/y_{SAVE_SUFFIX}.npy\", y, allow_pickle=False)\n",
    "        \n",
    "        print(\"Finished generating Xy\")\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "    \n",
    "        del bert_tokenizer\n",
    "        del bert_model\n",
    "        del sentence_model\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
