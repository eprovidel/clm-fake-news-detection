{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb070af-7e7d-457a-aec8-a2c1e5cb899e",
   "metadata": {},
   "source": [
    "**Manually set environment variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71496b0-50a4-4bb3-b5f1-cff4a16d1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FND_ROOT=/workspace/fnd-building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925731e-acb7-4918-8fb5-4e45a7c02bc9",
   "metadata": {},
   "source": [
    "**Set randomness sources first with original seed, for full reproducibility of results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f74d6a-8b6d-4915-938d-94516effc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97148915-ab05-471e-ae0d-f152f4d5a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_SEED = 30082010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a931a-caa4-4576-b387-ea34b4fdee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(OG_SEED)\n",
    "torch.manual_seed(OG_SEED)\n",
    "random.seed(OG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c9fe0-9c44-4499-ac69-8bbf0991c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be6d9b-21b1-4457-8446-7755041ebecf",
   "metadata": {},
   "source": [
    "**Configuration constants.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f272cbe-13a1-4138-9e21-ea3d63200328",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = 0\n",
    "FND_ROOT=%env FND_ROOT\n",
    "PLURALISMO_ROOT=f'{FND_ROOT}/datasets/datasets-fnd-pluralismo/OnlyRepliesTree'\n",
    "TAG = 'TF'\n",
    "SEQ_PADDING = True\n",
    "\n",
    "EMBEDDINGS_ROOT = f\"{FND_ROOT}/experiments/embeddings\"\n",
    "RUN_SUFFIX = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2113f11-8658-4be6-b0c9-914b7a8d8809",
   "metadata": {},
   "source": [
    "**Clear cuda cache and perform garbage collection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393da8b0-3370-4de8-a456-5f9080d06b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd283c85-845c-49db-80a4-1ecd862c5eea",
   "metadata": {},
   "source": [
    "**Setup CUDA device if GPU is available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc9e59-f673-450f-bbbb-51fa2b1acad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{CUDA_DEVICE}\" if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(CUDA_DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b4e70-d558-471c-9159-807fe790f0c3",
   "metadata": {},
   "source": [
    "**Configure ekphrasis text preprocessor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a0cb9-99e3-45ee-947a-a6692b253349",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    \n",
    "    # terms that will be normalized\n",
    "    normalize=[\n",
    "        'url',\n",
    "        'email',\n",
    "        'percent',\n",
    "        'money',\n",
    "        'phone',\n",
    "        'user', \n",
    "        'time',\n",
    "        'date',\n",
    "        'number'\n",
    "    ],\n",
    "    \n",
    "    # terms that will be annotated\n",
    "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\", 'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    # corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    # unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03284ba9-016d-4849-8564-8fea645901eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAllPosts(PLURALISMO_ROOT):\n",
    "        \"\"\"\n",
    "        Carga todos los posts en formato json desde el directorio ./post.\n",
    "        \n",
    "        Retorna:\n",
    "        \n",
    "        - all_posts, diccionario indexado por tweet id\n",
    "        - labeled_posts, diccionaro con tweets etiquetados\n",
    "        - number_of_tweets\n",
    "        \"\"\"\n",
    "        \n",
    "        def parseTwitterTree(tree_file):\n",
    "            tree_data = list()\n",
    "            for line in tree_file:\n",
    "                f_first_part, second_part = line.split('->')                \n",
    "                f, first_part = f_first_part.split(\":\")\n",
    "                first_part = first_part.strip()\n",
    "                first_part = first_part.replace(\"'\", \"\\\"\")\n",
    "                tree_data.append(json.loads(first_part))\n",
    "            return tree_data\n",
    "        \n",
    "        ### Obtener diccionario con todos los posts\n",
    "        all_posts = {}\n",
    "        labels = {}\n",
    "    \n",
    "        subtrees = ['false', 'true']\n",
    "        subtrees_count = {\n",
    "            'false': 0,\n",
    "            'true': 0,\n",
    "            'imprecise': 0\n",
    "        }\n",
    "        print(\"all_posts before:\")\n",
    "        print(len(all_posts))\n",
    "        print(all_posts)\n",
    "\n",
    "        def retrieve_replies(tweet_info, accumulator):\n",
    "            if \"replies\" not in tweet_info or len(tweet_info[\"replies\"]) == 0:\n",
    "                return accumulator\n",
    "            else:\n",
    "                new_accumulator = []\n",
    "                for tweet_reply in tweet_info[\"replies\"]:\n",
    "                    reply_id = tweet_reply[\"id\"]\n",
    "                    new_accumulator += retrieve_replies(tweet_reply, [tweet_reply])\n",
    "                return accumulator + new_accumulator\n",
    "\n",
    "        for subtree_label in subtrees:            \n",
    "            subtree_file_list = os.listdir(os.path.join(PLURALISMO_ROOT, subtree_label))\n",
    "            print(f\"Working with folder {subtree_label} with {len(subtree_file_list)} files\")\n",
    "            for file in subtree_file_list:\n",
    "                if file.endswith(\".json\") and not file.endswith(\"_minf.json\"):\n",
    "                    print(f\"Working with file {file}\")\n",
    "                    try:        \n",
    "                        with open(os.path.join(PLURALISMO_ROOT, subtree_label, file), 'r') as f:\n",
    "                            tweet_id  = file.split(\"_\")[1].split(\".\")[0]\n",
    "                            tweet_dic = json.load(f)\n",
    "                            for tweet_info in tweet_dic[\"data\"]:\n",
    "                                if \"conversation_id\" in tweet_info:\n",
    "                                    if tweet_info[\"conversation_id\"] == str(tweet_id):\n",
    "                                        all_posts[tweet_id] = tweet_info\n",
    "                            \n",
    "                            assert all_posts[tweet_id] is not None\n",
    "\n",
    "                            nested_replies = retrieve_replies(all_posts[tweet_id], [])\n",
    "                            print(f\"Got {len(nested_replies)} nested replies\")\n",
    "                            for reply in nested_replies:\n",
    "                                all_posts[reply[\"id\"]] = reply\n",
    "                            \n",
    "                            subtrees_count[subtree_label] += 1\n",
    "                            labels[tweet_id] = subtree_label\n",
    "                            \n",
    "                    except Exception as exc:\n",
    "                        print(\"EXC:\")\n",
    "                        print(exc)\n",
    "\n",
    "        print(\"all_posts after:\")\n",
    "        print(len(all_posts))\n",
    "        print(\"Tweets etiquetados      : \", len(labels), \" \", subtrees_count)\n",
    "\n",
    "        seqs_lens = []\n",
    "        labeled_posts = {}\n",
    "        number_of_tweets = 0\n",
    "        number_of_retweets = 0\n",
    "        number_of_invalid_tweets = 0\n",
    "        no_in_data = 0\n",
    "        opened_files = 0\n",
    "    \n",
    "        for idx, (tweet_id, subtree_label) in enumerate(labels.items()):\n",
    "            try:\n",
    "                if  tweet_id in all_posts:\n",
    "                    tree_path = os.path.join(PLURALISMO_ROOT, subtree_label, f\"treefile_{tweet_id}_minf.json\")\n",
    "                    with open(tree_path) as tree_file:\n",
    "                        opened_files += 1\n",
    "                        print(f\"File {tree_path} opened correctly\")\n",
    "                        \n",
    "                        tree_data = parseTwitterTree(tree_file)\n",
    "\n",
    "                        # ### Remover retweets\n",
    "                        first = tree_data[0]\n",
    "                        \n",
    "                        without_rt = list(filter(lambda t: t[0] != tweet_id, tree_data[1:]))\n",
    "                        number_of_retweets = number_of_retweets + (len(tree_data[1:]) - len(without_rt))                        \n",
    "                        only_valid = list(filter(lambda t: t[0] in all_posts, without_rt))                        \n",
    "                        number_of_invalid_tweets = number_of_invalid_tweets + (len(without_rt) - len(only_valid))\n",
    "                        seqs_lens.append(len(only_valid))\n",
    "                        \n",
    "                        labeled_posts[tweet_id] = (labels[tweet_id], [first] + only_valid)\n",
    "                        number_of_tweets = number_of_tweets + 1                \n",
    "                else:\n",
    "                     no_in_data = no_in_data + 1  \n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        assert opened_files == len(labels)\n",
    "\n",
    "        print(\"no_in_data              : \", no_in_data) ## están etiquetados, pero no en los post\n",
    "        print(\"number_of_tweets        : \", number_of_tweets)        \n",
    "        print(\"all_posts               : \", len(all_posts))\n",
    "        print(\"number_of_retweets      : \", number_of_retweets) ## En árbol de propagación\n",
    "        print(\"number_of_invalid_tweets: \", number_of_invalid_tweets) ## En árbol de propagación\n",
    "        \n",
    "        #La red neuronal necesita un tamaño fijo para la secuencia (datos de entrada)\n",
    "        #¿Que largo de secuencia utilizar?\n",
    "        counts = np.bincount(seqs_lens) ## seqs_len sólo de los 753\n",
    "        mode_seq_len = np.argmax(counts)\n",
    "        mean_seq_len = int(np.mean(seqs_lens))\n",
    "        min__seq_len = min(seqs_lens)\n",
    "        max__seq_len = max(seqs_lens) \n",
    "\n",
    "        print(\"len(seqs_lens)   : \", len(seqs_lens))\n",
    "        print(\"min__seq_len: \", min__seq_len)\n",
    "        print(\"max__seq_len: \", max__seq_len)\n",
    "        print(\"mean_seq_len: \", mean_seq_len)\n",
    "        print(\"mode_seq_len: \", mode_seq_len)\n",
    "\n",
    "        tree_max_num_seq = mean_seq_len\n",
    "        \n",
    "        return (all_posts, labeled_posts, number_of_tweets, tree_max_num_seq, seqs_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a21dd-554e-41b0-8c50-b619301d1eeb",
   "metadata": {},
   "source": [
    "**Setup papermill parameters**. The cell below must be tagged with the 'parameters' tag. See: https://papermill.readthedocs.io/en/latest/usage-parameterize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bba25-3538-4f90-ad8f-8373bda4b5ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# papermill parameters\n",
    "\n",
    "## Must be set to True when runnin via papermill.\n",
    "PAPERMILL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35d895-f015-4bf7-a925-c77999af084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"CHECKPOINT\": \"bert-base-uncased\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f8a0c-8524-4792-98b9-1f240a9d0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PAPERMILL:\n",
    "    print(\"Importing plain tqdm\")\n",
    "    from tqdm import tqdm    \n",
    "else:\n",
    "    print(\"Importing auto tqdm\")\n",
    "    from tqdm.auto import tqdm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9a95b-5653-49bf-81a4-072a210f14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_posts, \n",
    "    labeled_posts, \n",
    "    number_of_tweets, \n",
    "    tree_max_num_seq, \n",
    "    seqs_lens\n",
    ") = loadAllPosts(PLURALISMO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57b8ad-8dcf-412e-b4c1-ae13b5dbe2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_seqs_lens = np.array(list(filter(lambda x: x < 320, seqs_lens)))\n",
    "tree_max_num_seq = int(np.floor(np.mean(regular_seqs_lens)))\n",
    "tree_max_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec90e7-fdf1-4e65-af79-13df9449323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(seqs_lens)\n",
    "plt.show()\n",
    "print(np.mean(seqs_lens))\n",
    "plt.boxplot(regular_seqs_lens)\n",
    "plt.show()\n",
    "print(np.mean(regular_seqs_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335f362-ab28-409f-8a6a-6a2b4474f7ce",
   "metadata": {},
   "source": [
    "### Labels: true, false (omit 'imprecise' label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c28cd-cbe8-4264-8461-502c0b465145",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['true', 'false']\n",
    "encoded_labels = [0, 1, 2]\n",
    "idx2label = categories\n",
    "label2idx = dict(zip(idx2label, encoded_labels))\n",
    "num_categories = len(categories)\n",
    "\n",
    "print(f\"Number of categories: {num_categories}\")\n",
    "print(f\"Labels to indices: {label2idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed458d-8bd2-445b-944e-ae851dbef0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples_per_label():\n",
    "    label_list = [v[0] for k,v in labeled_posts.items()]\n",
    "    for label in idx2label:\n",
    "        idx = label2idx[label]\n",
    "        qty = len(list(filter(lambda l: l == label, label_list)))\n",
    "        print(f\"Label {idx} ({label}): {qty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33c39e-7f3a-45fc-9b47-4c85982473c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_examples_per_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb7634-b379-41de-8d1c-c411101c032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDER = None\n",
    "EMBEDDER_STRATEGY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97997a7-787c-4aed-b66d-7a99460e0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_XY_with_BERT(\n",
    "    _all_posts,\n",
    "    _emb_size,\n",
    "    _number_of_tweets,\n",
    "    _labeled_posts,\n",
    "    _tree_max_num_seq,\n",
    "    _categories,\n",
    "    _bert_tokenizer,\n",
    "    _bert_model,\n",
    "    _sentence_model\n",
    "):\n",
    "    \"\"\"\n",
    "     Generate X, Y matrices with embeddings ready to be applied on a neural netowrk.\n",
    "    In addition, it returns the list of words in post that are not found\n",
    "    in the given vocabulary.\n",
    "\n",
    "    It relies on global variables EMBEDDER and EMBEDDER_STRATEGY:\n",
    "    \n",
    "    - if EMBEDDER is 'RAW_BERT', then the _bert_tokenizer and _bert_model parameters are used\n",
    "    to generate the embeddings.\n",
    "\n",
    "    - if EMBEDDER is 'SENTENCE_BERT': the _sentence_model parameter is used to generate\n",
    "    the embeddings.    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X, Y, words_not_in_model)\n",
    "    \"\"\"\n",
    " \n",
    "\n",
    "    def to_category_vector(_category, _categories):\n",
    "        vector = np.zeros(len(_categories)).astype(np.float32)\n",
    "        for i in range(len(_categories)):\n",
    "            if _categories[i] == _category:\n",
    "                vector[i] = 1.0\n",
    "                break\n",
    "        return vector\n",
    "\n",
    "    ## padding al final, con empty\n",
    "    def padOrTruncate(empty_tensor, max_num, orig_tensor):\n",
    "        if not SEQ_PADDING:\n",
    "            return orig_tensor\n",
    "\n",
    "        len_orig_tensor = orig_tensor.size(0)\n",
    "\n",
    "        if len_orig_tensor > max_num:\n",
    "            # Truncate\n",
    "            result = orig_tensor[:max_num]\n",
    "        elif len_orig_tensor <= max_num:\n",
    "            # Pad\n",
    "            repeats = empty_tensor.repeat(max_num - len(orig_tensor), 1)\n",
    "            result = torch.cat((orig_tensor, repeats))\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def generateBERTEmbedding(docTexts):\n",
    "        global EMBEDDER\n",
    "        global EMBEDDER_STRATEGY\n",
    "        result = []\n",
    "\n",
    "        # print(f\"generate bert embedding {len(docTexts)}\")\n",
    "        # print(f\"len docTexts: {len(docTexts)}\")\n",
    "        for t in docTexts:\n",
    "            # print(f\"len text: {len(t)}\")\n",
    "\n",
    "            preprocessed_text = \" \".join(text_processor.pre_process_doc(t))\n",
    "           \n",
    "\n",
    "            # Default strategy: RAW-BERT\n",
    "            if EMBEDDER is None:\n",
    "                EMBEDDER = \"RAW_BERT\"\n",
    "\n",
    "            assert EMBEDDER in [\"RAW_BERT\", \"SENTENCE_BERT\"]\n",
    "\n",
    "            if EMBEDDER == \"RAW_BERT\":\n",
    "\n",
    "                tokenized = _bert_tokenizer.encode_plus(\n",
    "                    preprocessed_text,\n",
    "                    padding=False,\n",
    "                    truncation=True,\n",
    "                )\n",
    "\n",
    "                ids = torch.LongTensor(tokenized[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "                mask = torch.LongTensor(tokenized[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "                type_ids = (\n",
    "                    torch.LongTensor(tokenized[\"token_type_ids\"]).unsqueeze(0).to(device)\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    # https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.BaseModelOutput\n",
    "                    out = _bert_model(\n",
    "                        input_ids=ids, attention_mask=mask, token_type_ids=type_ids\n",
    "                    )\n",
    "                hidden_states = out[\"hidden_states\"]\n",
    "\n",
    "                # Default strategy for RAW-BERT embedder: embedding\n",
    "                if EMBEDDER_STRATEGY is None:\n",
    "                    EMBEDDER_STRATEGY = \"embedding\"\n",
    "\n",
    "                assert EMBEDDER_STRATEGY in [\"embedding\", \"pooler\", \"second_to_last\", \"sum_four_last\"]\n",
    "\n",
    "                if EMBEDDER_STRATEGY == \"embedding\":    \n",
    "                    ## Use embedding output layer\n",
    "                    embedding = hidden_states[0].cpu().detach()\n",
    "                    mean_embedding = torch.mean(embedding, dim=1)\n",
    "                    result.append(mean_embedding)\n",
    "                elif EMBEDDER_STRATEGY == \"pooler\":    \n",
    "                    ## Use pooler output\n",
    "                    embedding = out[\"pooler_output\"]\n",
    "                    result.append(embedding)\n",
    "                elif EMBEDDER_STRATEGY == \"second_to_last\":    \n",
    "                    ## Second-to-last hidden layer\n",
    "                    embedding = hidden_states[2].cpu().detach()\n",
    "                    mean_embedding = torch.mean(embedding, dim=1)\n",
    "                    result.append(mean_embedding)\n",
    "                elif EMBEDDER_STRATEGY == \"sum_four_last\":    \n",
    "                    ## Sum last four hidden\n",
    "                    last_four_layers = [torch.mean(hidden_states[i], dim=1) for i in (-1, -2, -3, -4)]\n",
    "                    tensor_last_four_layers = torch.stack(last_four_layers).squeeze(1)                    \n",
    "                    sum_hidden_states = torch.sum(tensor_last_four_layers, dim=0).cpu().detach()\n",
    "                    result.append(sum_hidden_states)\n",
    "\n",
    "            elif EMBEDDER == \"SENTENCE_BERT\":\n",
    "\n",
    "                # Default strategy for SENTENCE-BERT embedder: default\n",
    "                if EMBEDDER_STRATEGY is None:\n",
    "                    EMBEDDER_STRATEGY = \"default\"\n",
    "                \n",
    "                assert EMBEDDER_STRATEGY in [\"default\"]\n",
    "                embedding = torch.Tensor(sentence_model.encode(preprocessed_text)).to(device)\n",
    "                result.append(embedding)\n",
    "                \n",
    "\n",
    "        result = torch.stack(result).squeeze(1).cpu().detach()\n",
    "        return result\n",
    "\n",
    "    empty_tensor = torch.zeros([1, _emb_size])\n",
    "    _num_categories = len(_categories)\n",
    "\n",
    "    ## Calcula AWE de cada árbol\n",
    "    print(\"Pre labeled_posts_awe\")\n",
    "    labeled_posts_awe = {\n",
    "        k: (\n",
    "            v[0],\n",
    "            generateBERTEmbedding(list(map(lambda x: _all_posts[x[0]][\"text\"], v[1][0:_tree_max_num_seq]))),\n",
    "        )\n",
    "        for k, v in _labeled_posts.items()\n",
    "    }\n",
    "    print(\"Post labeled_posts_awe\")\n",
    "\n",
    "    ## Realiza padding o truncate a las secuencias\n",
    "    print(\"Pre padded_labeled_posts_awe\")\n",
    "    padded_labeled_posts_awe = {\n",
    "        k: (v[0], padOrTruncate(empty_tensor, _tree_max_num_seq, v[1]))\n",
    "        for k, v in labeled_posts_awe.items()\n",
    "    }\n",
    "    print(\"Post padded_labeled_posts_awe\")\n",
    "\n",
    "    # Genera los datos X e Y para alimentar el modelo de red neuronal\n",
    "    # Inicialmente con ceros y con la forma adecuada.\n",
    "    X = np.zeros(shape=(_number_of_tweets, _tree_max_num_seq, _emb_size)).astype(\n",
    "        np.float32\n",
    "    )\n",
    "    Y = np.zeros(shape=(_number_of_tweets, _num_categories)).astype(np.float32)\n",
    "\n",
    "    # Asigna al vector X los datos correspondientes\n",
    "    for idx, (tweet_id, tweet_data) in enumerate(\n",
    "        list(padded_labeled_posts_awe.items())\n",
    "    ):\n",
    "        for jdx, tweet_d in enumerate(tweet_data[1]):\n",
    "            if jdx == _tree_max_num_seq:\n",
    "                break\n",
    "            else:\n",
    "                X[idx, jdx, :] = tweet_d\n",
    "\n",
    "    # Asigna al vector Y los datos correspondientes\n",
    "    for idx, (tweet_id, tweet_data) in enumerate(\n",
    "        list(padded_labeled_posts_awe.items())\n",
    "    ):\n",
    "        Y[idx, :] = to_category_vector(tweet_data[0], _categories)\n",
    "\n",
    "    print(\"X.shape: \", np.shape(X))\n",
    "    print(\"Y.shape: \", np.shape(Y))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b690be4-3f24-4d7a-a14f-8586aa046ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [\n",
    "    (\"bert-base-multilingual-uncased\", \"BASE0\"),\n",
    "    (\"eprovidel/CLM01_v2_BotEN\", \"CLM01\"),\n",
    "    (\"eprovidel/CLM02_v2_BotES\", \"CLM02\"),\n",
    "    (\"eprovidel/CLM03_v2_StanceEN\", \"CLM03\"),\n",
    "    (\"eprovidel/CLM04_v2_BotEN_BotES\", \"CLM04\"),\n",
    "    (\"eprovidel/CLM05_v2_BotES_BotEN\", \"CLM05\"),\n",
    "    (\"eprovidel/CLM06_v2_BotEN_StanceEN\", \"CLM06\"),\n",
    "    (\"eprovidel/CLM07_v2_BotES_StanceEN\", \"CLM07\"),\n",
    "    (\"eprovidel/CLM08_v2_StanceEN_BotEN\", \"CLM08\"),\n",
    "    (\"eprovidel/CLM09_v2_StanceEN_BotES\", \"CLM09\"),\n",
    "    (\"eprovidel/CLM10_v2_BotEN_BotES_StanceEN\", \"CLM10\"),\n",
    "    (\"eprovidel/CLM11_v2_BotES_BotEN_StanceEN\", \"CLM11\"),\n",
    "    (\"eprovidel/CLM12_v2_BotEN_StanceEN_BotES\", \"CLM12\"),\n",
    "    (\"eprovidel/CLM13_v2_BotES_StanceEN_BotEN\", \"CLM13\"),\n",
    "    (\"eprovidel/CLM14_v2_StanceEN_BotEN_BotES\", \"CLM14\"),\n",
    "    (\"eprovidel/CLM15_v2_StanceEN_BotES_BotEN\", \"CLM15\"),\n",
    "]\n",
    "\n",
    "embedders_strategy = [\n",
    "    (\"RAW_BERT\", \"embedding\"),\n",
    "    (\"RAW_BERT\", \"pooler\"),\n",
    "    (\"RAW_BERT\", \"second_to_last\"),\n",
    "    (\"RAW_BERT\", \"sum_four_last\"),\n",
    "    (\"SENTENCE_BERT\", \"default\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67769bd-d713-41b9-8c50-bb89bc588a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "await send_telegram_notification(telegram_token, chat_id, \"Started generating Pluralismo embeddings...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e06e9d-a7b3-4e9b-8d01-99c08dd01bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _embedder, _strategy in embedders_strategy:\n",
    "    EMBEDDER = _embedder\n",
    "    EMBEDDER_STRATEGY = _strategy\n",
    "    print(\">\" * 80)\n",
    "    print(f\"Embedder: {_embedder} with strategy: {_strategy}\")\n",
    "    \n",
    "    for _checkpoint, _shortcheckpoint in checkpoints:\n",
    "        SAVE_SUFFIX = f\"PLR_{_shortcheckpoint}_{TAG}_{EMBEDDER}_{EMBEDDER_STRATEGY}\"\n",
    "        print(\"#\" * 60)\n",
    "        print(f\"Creating embedding: {SAVE_SUFFIX}\")\n",
    "\n",
    "        emb_size = None\n",
    "\n",
    "        sentence_model = None\n",
    "        if EMBEDDER == 'SENTENCE_BERT':\n",
    "            word_embedding_model = models.Transformer(_checkpoint, max_seq_length=512)            \n",
    "            emb_size = word_embedding_model._modules['auto_model'].config.hidden_size            \n",
    "            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "            sentence_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "        bert_model = None\n",
    "        bert_tokenizer = None        \n",
    "        if EMBEDDER == 'RAW_BERT':    \n",
    "            bert_tokenizer = AutoTokenizer.from_pretrained(_checkpoint)\n",
    "            bert_model = AutoModel.from_pretrained(_checkpoint, output_hidden_states=True)\n",
    "            bert_model.to(device);\n",
    "            emb_size = bert_model.config.hidden_size\n",
    "        \n",
    "        print(\"Recomputing Xy\")\n",
    "        X, y = generate_XY_with_BERT(\n",
    "            all_posts,\n",
    "            emb_size,\n",
    "            number_of_tweets,\n",
    "            labeled_posts,\n",
    "            tree_max_num_seq,\n",
    "            categories,\n",
    "            bert_tokenizer,\n",
    "            bert_model,\n",
    "            sentence_model,\n",
    "        )\n",
    "        np.save(f\"{EMBEDDINGS_ROOT}/X_{SAVE_SUFFIX}.npy\", X, allow_pickle=False)\n",
    "        np.save(f\"{EMBEDDINGS_ROOT}/y_{SAVE_SUFFIX}.npy\", y, allow_pickle=False)\n",
    "        \n",
    "        print(\"Finished generating Xy\")\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "    \n",
    "        del bert_tokenizer\n",
    "        del bert_model\n",
    "        del sentence_model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c1d03-4033-4abc-87c2-0d8decf83e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
