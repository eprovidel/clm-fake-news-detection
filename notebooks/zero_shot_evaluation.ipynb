{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual setup of required environment variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FND_ROOT=/workspace/fnd-building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"FND_GRU_RNN_01_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_01_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_02_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_02_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_03_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_03_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_04_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_04_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_05_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_05_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_06_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_06_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_07_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_07_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_08_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_08_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_09_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_09_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_10_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_10_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_11_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_11_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_12_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_12_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_13_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_13_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_14_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_14_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_15_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_15_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_16_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_16_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_17_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_17_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_18_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_18_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_19_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_19_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_20_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_20_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_21_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_21_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_22_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_22_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_23_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_23_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_24_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_24_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_25_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_25_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_26_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_26_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_27_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_27_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_28_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_28_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_29_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_29_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_30_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_30_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_31_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_31_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_32_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_32_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_33_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_33_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_34_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_34_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_35_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_35_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_36_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_36_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_37_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_37_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_38_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_38_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_39_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_39_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_40_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_40_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_41_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_41_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_42_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_42_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_43_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_43_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_44_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_44_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_45_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_45_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_46_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_46_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_47_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_47_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_48_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_48_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_49_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_49_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_50_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_50_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_51_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_51_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_52_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_52_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_53_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_53_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_54_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_54_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_55_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_55_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_56_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_56_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_57_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_57_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_58_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_58_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_59_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_59_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_60_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_60_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_61_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_61_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_62_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_62_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_63_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_63_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_64_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_64_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_65_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_65_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_66_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_66_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_67_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_67_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_68_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_68_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_69_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_69_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_70_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_70_EPOCH_DATA_{}\",\n",
    "    \"FND_GRU_RNN_71_EPOCH_{}\",\n",
    "    \"FND_GRU_RNN_71_EPOCH_DATA_{}\",     \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_and_epoch_data = [\n",
    "    400, \n",
    "    433, \n",
    "    406,\n",
    "    453,\n",
    "    187,\n",
    "    280,\n",
    "    364,\n",
    "    574,\n",
    "    393,\n",
    "    430, \n",
    "    351,\n",
    "    351,\n",
    "    63,\n",
    "    940, \n",
    "    215,\n",
    "    231, \n",
    "    402, \n",
    "    430, \n",
    "    820, \n",
    "    978,\n",
    "    376, \n",
    "    376, \n",
    "    141, \n",
    "    238, \n",
    "    229, \n",
    "    305, \n",
    "    393,\n",
    "    429, \n",
    "    995, \n",
    "    995, \n",
    "    355, \n",
    "    470, \n",
    "    133, \n",
    "    193, \n",
    "    261,\n",
    "    291,\n",
    "    375,\n",
    "    413,\n",
    "    377, \n",
    "    471,\n",
    "    152,\n",
    "    246,\n",
    "    168,\n",
    "    168,\n",
    "    373,\n",
    "    413,\n",
    "    951,\n",
    "    951,\n",
    "    345,\n",
    "    385,\n",
    "    147,\n",
    "    197,\n",
    "    231,\n",
    "    278,\n",
    "    409,\n",
    "    418,\n",
    "    318,\n",
    "    514,\n",
    "    180,\n",
    "    782,\n",
    "    374,\n",
    "    374,\n",
    "    393,\n",
    "    429,\n",
    "    391,\n",
    "    666,\n",
    "    122,\n",
    "    156,\n",
    "    233,\n",
    "    233,\n",
    "    393,\n",
    "    429,\n",
    "    711,\n",
    "    884,\n",
    "    356,\n",
    "    514,\n",
    "    240,\n",
    "    240, \n",
    "    397, \n",
    "    429,\n",
    "    990, \n",
    "    990, \n",
    "    339, \n",
    "    414, \n",
    "    141, \n",
    "    782, \n",
    "    295, \n",
    "    340,\n",
    "    393, \n",
    "    412, \n",
    "    321,\n",
    "    388,\n",
    "    121,\n",
    "    199,\n",
    "    246, \n",
    "    246,\n",
    "    393,\n",
    "    413,\n",
    "    985,\n",
    "    985,\n",
    "    348,\n",
    "    502,\n",
    "    132,\n",
    "    163,\n",
    "    240,\n",
    "    274,\n",
    "    393,\n",
    "    413,\n",
    "    937,\n",
    "    937,\n",
    "    316,\n",
    "    441,\n",
    "    119,\n",
    "    359,\n",
    "    204,\n",
    "    286,\n",
    "    393,\n",
    "    418,\n",
    "    666,\n",
    "    896,\n",
    "    333,\n",
    "    650,\n",
    "    296,\n",
    "    348,\n",
    "    364,\n",
    "    418,\n",
    "    648,\n",
    "    738,\n",
    "    333,\n",
    "    537,\n",
    "    147,\n",
    "    189,\n",
    "    233,\n",
    "    344,\n",
    "    376,\n",
    "    418,\n",
    "    344,\n",
    "    470,\n",
    "    188,\n",
    "    237,\n",
    "    417,\n",
    "    629,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_skip_epoch = [\n",
    "    \"FND_GRU_RNN_06_EPOCH_351\",\n",
    "    \"FND_GRU_RNN_11_EPOCH_376\",\n",
    "    \"FND_GRU_RNN_15_EPOCH_995\",\n",
    "    \"FND_GRU_RNN_22_EPOCH_168\",\n",
    "    \"FND_GRU_RNN_24_EPOCH_951\",\n",
    "    \"FND_GRU_RNN_31_EPOCH_374\",\n",
    "    \"FND_GRU_RNN_35_EPOCH_233\",\n",
    "    \"FND_GRU_RNN_39_EPOCH_240\",\n",
    "    \"FND_GRU_RNN_41_EPOCH_990\",\n",
    "    \"FND_GRU_RNN_48_EPOCH_246\",\n",
    "    \"FND_GRU_RNN_50_EPOCH_985\",\n",
    "    \"FND_GRU_RNN_55_EPOCH_937\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [t[0].format(t[1]) for t in zip(models, epoch_and_epoch_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(models_to_evaluate) == len(models) == len(epoch_and_epoch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = [m for m in models_to_evaluate if m not in models_to_skip_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(models_to_evaluate) == (len(models) - len(models_to_skip_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\n",
    "    # 1\n",
    "    \"BASE0_TF_RAW_BERT_embedding\",\n",
    "    \"BASE0_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 2\n",
    "    \"BASE0_TF_RAW_BERT_second_to_last\",\n",
    "    \"BASE0_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 3\n",
    "    \"BASE0_TF_RAW_BERT_sum_four_last\",\n",
    "    \"BASE0_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 4\n",
    "    \"BASE0_TF_SENTENCE_BERT_default\",\n",
    "    \"BASE0_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 5\n",
    "    \"CLM01_TF_RAW_BERT_embedding\",\n",
    "    \"CLM01_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 6\n",
    "    \"CLM01_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 7\n",
    "    \"CLM01_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM01_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 8\n",
    "    \"CLM01_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM01_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 9\n",
    "    \"CLM02_TF_RAW_BERT_embedding\",\n",
    "    \"CLM02_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 10\n",
    "    \"CLM02_TF_RAW_BERT_pooler\",\n",
    "    \"CLM02_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 11\n",
    "    \"CLM02_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 12\n",
    "    \"CLM02_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM02_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 13\n",
    "    \"CLM02_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM02_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 14\n",
    "    \"CLM03_TF_RAW_BERT_embedding\",\n",
    "    \"CLM03_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 15\n",
    "    \"CLM03_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 16\n",
    "    \"CLM03_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM03_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 17\n",
    "    \"CLM03_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM03_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 18\n",
    "    \"CLM03_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM03_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 19\n",
    "    \"CLM04_TF_RAW_BERT_embedding\",\n",
    "    \"CLM04_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 20\n",
    "    \"CLM04_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM04_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 21\n",
    "    \"CLM04_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM04_TF_RAW_BERT_sum_four_last\",\n",
    "    \n",
    "    # 22\n",
    "    \"CLM04_TF_SENTENCE_BERT_default\",\n",
    "    \n",
    "    # 23\n",
    "    \"CLM05_TF_RAW_BERT_embedding\",\n",
    "    \"CLM05_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 24\n",
    "    \"CLM05_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 25\n",
    "    \"CLM05_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM05_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 26\n",
    "    \"CLM05_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM05_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 27\n",
    "    \"CLM05_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM05_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 28\n",
    "    \"CLM06_TF_RAW_BERT_embedding\",\n",
    "    \"CLM06_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 29\n",
    "    \"CLM06_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM06_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 30\n",
    "    \"CLM06_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM06_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 31\n",
    "    \"CLM06_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 32\n",
    "    \"CLM07_TF_RAW_BERT_embedding\",\n",
    "    \"CLM07_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 33\n",
    "    \"CLM07_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM07_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 34\n",
    "    \"CLM07_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM07_TF_RAW_BERT_sum_four_last\",    \n",
    "\n",
    "    # 35\n",
    "    \"CLM07_TF_SENTENCE_BERT_default\",    \n",
    "\n",
    "    # 36\n",
    "    \"CLM08_TF_RAW_BERT_embedding\",\n",
    "    \"CLM08_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 37\n",
    "    \"CLM08_TF_RAW_BERT_pooler\",\n",
    "    \"CLM08_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 38\n",
    "    \"CLM08_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM08_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 39\n",
    "    \"CLM08_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 40\n",
    "    \"CLM09_TF_RAW_BERT_embedding\",\n",
    "    \"CLM09_TF_RAW_BERT_embedding\",    \n",
    "\n",
    "    # 41\n",
    "    \"CLM09_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 42\n",
    "    \"CLM09_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM09_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 43\n",
    "    \"CLM09_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM09_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 44\n",
    "    \"CLM09_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM09_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 45\n",
    "    \"CLM10_TF_RAW_BERT_embedding\",\n",
    "    \"CLM10_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 46\n",
    "    \"CLM10_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM10_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 47\n",
    "    \"CLM10_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM10_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 48\n",
    "    \"CLM10_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 49\n",
    "    \"CLM11_TF_RAW_BERT_embedding\",\n",
    "    \"CLM11_TF_RAW_BERT_embedding\",    \n",
    "\n",
    "    # 50\n",
    "    \"CLM11_TF_RAW_BERT_pooler\",\n",
    "    \n",
    "    # 51\n",
    "    \"CLM11_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM11_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 52\n",
    "    \"CLM11_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM11_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 53\n",
    "    \"CLM11_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM11_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 54\n",
    "    \"CLM12_TF_RAW_BERT_embedding\",\n",
    "    \"CLM12_TF_RAW_BERT_embedding\",    \n",
    "\n",
    "    # 55\n",
    "    \"CLM12_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 56\n",
    "    \"CLM12_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM12_TF_RAW_BERT_second_to_last\",\n",
    "    \n",
    "    # 57\n",
    "    \"CLM12_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM12_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 58\n",
    "    \"CLM12_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM12_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 59\n",
    "    \"CLM13_TF_RAW_BERT_embedding\",\n",
    "    \"CLM13_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 60\n",
    "    \"CLM13_TF_RAW_BERT_pooler\",\n",
    "    \"CLM13_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 61\n",
    "    \"CLM13_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM13_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 62\n",
    "    \"CLM13_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM13_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 63\n",
    "    \"CLM14_TF_RAW_BERT_embedding\",\n",
    "    \"CLM14_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 64\n",
    "    \"CLM14_TF_RAW_BERT_pooler\",\n",
    "    \"CLM14_TF_RAW_BERT_pooler\",\n",
    "\n",
    "    # 65\n",
    "    \"CLM14_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM14_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 66\n",
    "    \"CLM14_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM14_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 67\n",
    "    \"CLM14_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM14_TF_SENTENCE_BERT_default\",\n",
    "\n",
    "    # 68\n",
    "    \"CLM15_TF_RAW_BERT_embedding\",\n",
    "    \"CLM15_TF_RAW_BERT_embedding\",\n",
    "\n",
    "    # 69\n",
    "    \"CLM15_TF_RAW_BERT_second_to_last\",\n",
    "    \"CLM15_TF_RAW_BERT_second_to_last\",\n",
    "\n",
    "    # 70\n",
    "    \"CLM15_TF_RAW_BERT_sum_four_last\",\n",
    "    \"CLM15_TF_RAW_BERT_sum_four_last\",\n",
    "\n",
    "    # 71\n",
    "    \"CLM15_TF_SENTENCE_BERT_default\",\n",
    "    \"CLM15_TF_SENTENCE_BERT_default\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(embeddings) == len(models_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = list(zip(models_to_evaluate, embeddings))\n",
    "print(len(model_embeddings))\n",
    "model_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define constants and import all randomness sources.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration constants.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = 0\n",
    "OG_SEED = 31011985\n",
    "\n",
    "FND_ROOT=%env FND_ROOT\n",
    "EMBEDDINGS_PREFIX=f\"{FND_ROOT}/experiments/embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(OG_SEED)\n",
    "torch.manual_seed(OG_SEED)\n",
    "random.seed(OG_SEED)\n",
    "run_seed = np.random.randint(0, 42069, size=1)[0]\n",
    "run_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All other imports.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup papermill parameters.** The cell below must be tagged with the 'parameters' tag. See: https://papermill.readthedocs.io/en/latest/usage-parameterize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PAPERMILL PARAMETERS\n",
    "PAPERMILL = False\n",
    "\n",
    "# Model-specific parameters\n",
    "EPOCHS = 1000\n",
    "\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-04\n",
    "\n",
    "GRU_NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "UNITS = 1024\n",
    "\n",
    "DROPOUT = 0.2\n",
    "INTERNAL_DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clear CUDA cache and perform garbage collection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup CUDA device if GPU is available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{CUDA_DEVICE}\" if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(CUDA_DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Constants\"\n",
    "RUN_PREFIX=datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "RUN_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"EPOCHS\": EPOCHS,    \n",
    "    \"TRAIN_BATCH_SIZE\": TRAIN_BATCH_SIZE,\n",
    "    \"VALID_BATCH_SIZE\": VALID_BATCH_SIZE,    \n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "    \"GRU_NUM_LAYERS\": GRU_NUM_LAYERS,\n",
    "    \"UNITS\": UNITS,\n",
    "    \"BIDIRECTIONAL\": BIDIRECTIONAL,\n",
    "    \"DROPOUT\": DROPOUT,\n",
    "    \"INTERNAL_DROPOUT\": INTERNAL_DROPOUT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PAPERMILL:\n",
    "    print(\"Importing plain tqdm\")\n",
    "    from tqdm import tqdm    \n",
    "else:\n",
    "    print(\"Importing auto tqdm\")\n",
    "    from tqdm.auto import tqdm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, examples, labels):\n",
    "        assert len(examples) == len(labels)\n",
    "        self.examples = examples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        example = self.examples[index]\n",
    "\n",
    "        # From one-hot encoded to categorical label\n",
    "        label = np.argmax(self.labels[index])\n",
    "        \n",
    "        return {\n",
    "            'example': torch.tensor(example),            \n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {\n",
    "    'batch_size': settings[\"VALID_BATCH_SIZE\"],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI-GRU torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FND_BI_GRU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional GRU for fake news classification.\n",
    "\n",
    "    Pytorch reimplementation of model in Providel&Mendoza (2020).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # _tree_max_num_seq,\n",
    "        _emb_size,\n",
    "        _units=200,\n",
    "        _num_layers=2,\n",
    "        _num_categories=2,\n",
    "        _bidirectional=True,\n",
    "        _internal_dropout=0.1,\n",
    "        _dropout=0.1,\n",
    "    ):\n",
    "        super(FND_BI_GRU, self).__init__()\n",
    "\n",
    "        self.input_size = _emb_size\n",
    "        self.hidden_size = _units\n",
    "        self.num_layers = _num_layers # GRU_NUM_LAYERS\n",
    "        self.output_size = _num_categories\n",
    "        self.bidirectional = _bidirectional # BIDIRECTIONAL\n",
    "        self.bidirectional_factor = 2 if self.bidirectional else 1\n",
    "\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional,\n",
    "            dropout=_internal_dropout,\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(_dropout)\n",
    "        self.fc = torch.nn.Linear(\n",
    "            self.hidden_size * self.bidirectional_factor, 1 # 1 because it's binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply GRU.\n",
    "        h0 = torch.zeros(\n",
    "            self.num_layers * self.bidirectional_factor, x.size(0), self.hidden_size\n",
    "        ).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "\n",
    "        # Apply dropout.\n",
    "        out = self.dropout(\n",
    "            out[:, -1, :]\n",
    "        )  \n",
    "\n",
    "        # Apply linear layer.\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testing_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    loss_acum = 0\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    N = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for iters, data in tqdm(\n",
    "            enumerate(testing_loader, 0), total=len(testing_loader)\n",
    "        ):\n",
    "            x = data[\"example\"].to(device)\n",
    "            targets = data[\"label\"].to(device)\n",
    "\n",
    "            # Make forward pass for prediction.\n",
    "            logits = model(x)\n",
    "            logits = logits.squeeze(1)\n",
    "            \n",
    "            loss = loss_fn(logits, targets.float())\n",
    "\n",
    "            # Compute accumulated loss for reporting progress.\n",
    "            loss_acum += loss.item()\n",
    "            N = N + 1\n",
    "\n",
    "            # Compute expected outputs vs model outputs for reporting progress.\n",
    "            # We use sigmoid activation as it is a binary classification problem.\n",
    "            fin_outputs.extend(torch.sigmoid(logits).cpu().detach().numpy())\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "\n",
    "    return loss_acum / N, np.array(fin_outputs), np.array(fin_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataframe to tabulate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(settings.keys())\n",
    "column_names = column_names + [\"seed\", \"modelname\", \"embedding\"]\n",
    "column_names = column_names + [\"test_loss\", \"test_accuracy\", \"test_f1_score_micro\", \"test_f1_score_macro\"]\n",
    "column_names = column_names + [\"test_f1_score_label0\", \"test_f1_score_label1\"]\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results = pd.DataFrame(columns=column_names)\n",
    "run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python send_bot.py \"Pluralismo zero shot started!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_units = settings[\"UNITS\"]\n",
    "_num_layers = settings[\"GRU_NUM_LAYERS\"]\n",
    "_bidirectional = settings[\"BIDIRECTIONAL\"]\n",
    "_internal_dropout = settings[\"INTERNAL_DROPOUT\"]\n",
    "_dropout = settings[\"DROPOUT\"]\n",
    "\n",
    "for modelname, embedding in model_embeddings:\n",
    "    print(f\"###### MODEL: {modelname}\")\n",
    "    print(f\"###### EMBEDDING: {embedding}\")\n",
    "    snapshot_download(f\"eprovidel/{modelname}\", local_dir='./repo')\n",
    "\n",
    "    # ############################################\n",
    "    # Model evaluation happens here\n",
    "\n",
    "    # ############################################\n",
    "    # Load embedding data\n",
    "    X = None\n",
    "    y = None\n",
    "    \n",
    "    print(f\"Loading Xy from snapshot {embedding}\")\n",
    "    \n",
    "    X = np.load(f\"{EMBEDDINGS_PREFIX}/X_PLR_{embedding}.npy\")\n",
    "    y = np.load(f\"{EMBEDDINGS_PREFIX}/y_PLR_{embedding}.npy\")\n",
    "    \n",
    "    assert X is not None\n",
    "    assert y is not None\n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    print(f\"Loaded Xy from snapshot: {embedding}\")\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    # Remove label 2\n",
    "    #print(type(y))\n",
    "    #label2 = np.logical_not(np.all(y == [0, 0, 1], axis=1))\n",
    "    #print(len(label2))\n",
    "    #print(label2)\n",
    "\n",
    "    #y = y[label2]\n",
    "    print(y.shape)\n",
    "    \n",
    "    #X = X[label2]\n",
    "    print(X.shape)\n",
    "    \n",
    "\n",
    "    _, tree_max_num_seq, emb_size = X.shape\n",
    "    _, num_categories = y.shape\n",
    "\n",
    "    model = FND_BI_GRU(\n",
    "        _emb_size=emb_size,\n",
    "        _units=_units,\n",
    "        _num_layers=_num_layers,\n",
    "        _num_categories=num_categories,\n",
    "        _bidirectional=_bidirectional,\n",
    "        _internal_dropout=_internal_dropout,\n",
    "        _dropout=_dropout\n",
    "    )\n",
    "    print(model)\n",
    "\n",
    "    # ####### LOAD PRE-TRAINED WEIGHTS #######\n",
    "    pattern = f\"./repo/{modelname}*.pt\"\n",
    "    for filename in glob.glob(pattern, recursive=False):\n",
    "        print(f\"PRE-TRAINED WEIGHTS FOUND AT: {filename}\")\n",
    "        model.load_state_dict(torch.load(filename))\n",
    "        break\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    test_set = CustomDataset(X, y)\n",
    "    testing_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "    test_loss, test_outputs, test_targets = validate(model, testing_loader)\n",
    "    test_outputs_bin = (test_outputs >= 0.5) # np.argmax(test_outputs, axis=1)\n",
    "    test_accuracy = metrics.accuracy_score(test_targets, test_outputs_bin)\n",
    "    test_f1_score_micro = metrics.f1_score(test_targets, test_outputs_bin, average='micro')\n",
    "    test_f1_score_macro = metrics.f1_score(test_targets, test_outputs_bin, average='macro')\n",
    "\n",
    "    # Per class f1_score_macro, when labels are binary.\n",
    "    test_f1_score_macro_binary0 = metrics.f1_score(test_targets, test_outputs_bin, average='binary', pos_label=0)\n",
    "    test_f1_score_macro_binary1 = metrics.f1_score(test_targets, test_outputs_bin, average='binary', pos_label=1)\n",
    "\n",
    "    print(f\"TEST Accuracy Score = {test_accuracy}\")\n",
    "    print(f\"TEST F1 Score (Micro) = {test_f1_score_micro}\")\n",
    "    print(f\"TEST F1 Score (Macro) = {test_f1_score_macro}\")\n",
    "    print(f\"TEST F1 Score (Macro) for label 0 = {test_f1_score_macro_binary0}\")\n",
    "    print(f\"TEST F1 Score (Macro) for label 1 = {test_f1_score_macro_binary1}\")\n",
    "    print(f\"TEST loss: {test_loss}\")\n",
    "\n",
    "    results_row = {\n",
    "        'seed': run_seed,\n",
    "        'modelname': modelname,\n",
    "        'embedding': embedding,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_f1_score_micro': test_f1_score_micro,\n",
    "        'test_f1_score_macro': test_f1_score_macro,\n",
    "        'test_f1_score_label0': test_f1_score_macro_binary0,\n",
    "        'test_f1_score_label1': test_f1_score_macro_binary1,\n",
    "        **settings,\n",
    "    }\n",
    "\n",
    "    run_results = pd.concat([run_results, pd.DataFrame([results_row])], ignore_index=True)\n",
    "\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in Excel file\n",
    "EXCEL_OUTPUT = f\"PLURALISMO_ZERO_SHOT_RUN_{RUN_PREFIX}.xlsx\"\n",
    "run_results.to_excel(EXCEL_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python send_bot.py \"Pluralismo zero shot finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
